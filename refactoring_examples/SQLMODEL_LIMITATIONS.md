# SQLModel/SQLAlchemy Limitations: When Raw SQL Is Still Needed

**Date:** October 7, 2025
**Analysis:** Edge cases where SQLModel/SQLAlchemy won't work

---

## Executive Summary - **UPDATED with Inspector Findings!**

**MAJOR UPDATE:** SQLAlchemy Inspector works with DuckDB! This significantly reduces raw SQL needs.

SQLModel/SQLAlchemy can now replace **~85-98%** of raw SQL in Mock Spark. Only **4 scenarios** where raw SQL is still necessary:

1. âœ… ~~**Database Metadata Queries**~~ - **NOW WORKS with Inspector!**
2. âš ï¸ **Fully Dynamic Schemas** (use SQLAlchemy Core, not raw SQL)
3. âœ‹ **SQL Parser Implementation** (need to handle arbitrary SQL)
4. âœ‹ **DuckDB-Specific Features** (PRAGMA, extensions only)
5. âš¡ **Performance-Critical Bulk Operations** (optional: raw SQL faster)
6. âš ï¸ **Complex Window Functions** (SQLAlchemy works, but verbose)
7. âš ï¸ **Dynamic Table Names** (use Inspector reflection)

**Legend:**
- âœ… = **SOLVED** - Can now use SQLAlchemy
- âœ‹ = **Must use raw SQL** (no SQLAlchemy equivalent)
- âš ï¸ = **Can use SQLAlchemy** with workarounds
- âš¡ = **Optional raw SQL** for performance only

**Key Finding:** We can eliminate **3-5% MORE** raw SQL than originally estimated!

---

## Detailed Analysis

### 1. âœ… Database Metadata Queries - **UPDATED: NOW WORKS WITH SQLAlchemy!**

**Previous Issue (RESOLVED):** We thought SQLAlchemy didn't support DuckDB metadata queries.

**NEW FINDING:** SQLAlchemy Inspector **WORKS PERFECTLY** with DuckDB via `duckdb-engine`!

**Examples - OLD vs NEW:**

#### List Tables
```python
# âŒ OLD WAY (raw SQL - no longer needed!)
def list_tables(self) -> List[str]:
    result = self.connection.execute("SHOW TABLES").fetchall()
    return [row[0] for row in result]

# âœ… NEW WAY (SQLAlchemy Inspector)
from sqlalchemy import inspect

def list_tables(self) -> List[str]:
    """List tables using SQLAlchemy Inspector."""
    inspector = inspect(self.engine)
    return inspector.get_table_names()
```

#### Check Table Exists
```python
# âŒ OLD WAY
def table_exists(self, table: str) -> bool:
    try:
        self.connection.execute(f"SELECT 1 FROM {table} LIMIT 1")
        return True
    except:
        return False

# âœ… NEW WAY (SQLAlchemy Inspector)
from sqlalchemy import inspect

def table_exists(self, table: str) -> bool:
    """Check table existence using Inspector."""
    inspector = inspect(self.engine)
    return inspector.has_table(table)
```

#### Get Column Metadata (DESCRIBE replacement)
```python
# âŒ OLD WAY
result = conn.execute(f"DESCRIBE {table_name}").fetchall()

# âœ… NEW WAY (SQLAlchemy Inspector)
from sqlalchemy import inspect

inspector = inspect(engine)
columns = inspector.get_columns(table_name)
# Returns: [{'name': 'id', 'type': INTEGER, 'nullable': True}, ...]
```

**Why This Works Now:**
- Mock Spark already has `duckdb-engine>=0.15.0` installed
- Provides full SQLAlchemy dialect support for DuckDB
- Inspector methods work perfectly with DuckDB

**Tested and Confirmed:** âœ… All Inspector methods work correctly!

**Recommendation:** âœ… **USE SQLAlchemy Inspector** for all metadata operations - it's type-safe, database-agnostic, and cleaner than raw SQL!

---

### 2. âœ‹ Fully Dynamic Schemas (Schema Unknown Until Runtime)

**Issue:** When table structure is completely determined by data at runtime.

**Example from codebase:**

#### `mock_spark/dataframe/sql_builder.py:292-347`
```python
def create_temp_table_sql(self, data: List[Dict[str, Any]]) -> str:
    """Create SQL to insert data into a temporary table."""
    if not data:
        return ""

    # âœ‹ DYNAMIC SCHEMA - Can't define SQLModel class at import time
    columns = []
    if data:
        for key in data[0].keys():
            # Infer type from sample data
            sample_value = data[0][key]
            if isinstance(sample_value, int):
                columns.append(f'"{key}" INTEGER')
            elif isinstance(sample_value, float):
                columns.append(f'"{key}" DOUBLE')
            # ... more types

    # Must use raw SQL because schema is completely dynamic
    create_sql = f"CREATE TABLE \"{self.table_name}\" ({', '.join(columns)})"
    return create_sql
```

**Why SQLModel doesn't work:**
- SQLModel classes must be defined at import time
- Can't create dynamic classes with `type()` for every operation
- Schema depends on data content, not predefined structure

**Solutions:**

#### Option 1: Use SQLAlchemy Core (dynamic tables) âœ…
```python
from sqlalchemy import Table, Column, Integer, String, Float, MetaData

def create_table_from_data(name: str, data: List[Dict], metadata: MetaData):
    """Create dynamic table using SQLAlchemy Core."""
    if not data:
        return None

    # Infer columns from data
    columns = []
    for key, value in data[0].items():
        if isinstance(value, int):
            columns.append(Column(key, Integer))
        elif isinstance(value, float):
            columns.append(Column(key, Float))
        else:
            columns.append(Column(key, String))

    # Create table dynamically
    table = Table(name, metadata, *columns)
    return table
```

#### Option 2: Dynamic SQLModel classes (advanced) âš ï¸
```python
from sqlmodel import SQLModel, Field
from typing import Optional

def create_dynamic_model(name: str, data: List[Dict]):
    """Create SQLModel class dynamically."""
    annotations = {}
    class_attrs = {"__tablename__": name, "table": True}

    for key, value in data[0].items():
        if isinstance(value, int):
            annotations[key] = Optional[int]
        elif isinstance(value, float):
            annotations[key] = Optional[float]
        else:
            annotations[key] = Optional[str]
        class_attrs[key] = Field(default=None)

    class_attrs["__annotations__"] = annotations

    # Create class dynamically
    DynamicModel = type(f"Dynamic_{name}", (SQLModel,), class_attrs)
    return DynamicModel
```

**Recommendation:** âœ… Use **SQLAlchemy Core** for fully dynamic schemas. It's designed for this use case.

---

### 3. âœ‹ SQL Parser Implementation (Arbitrary SQL Execution)

**Issue:** When implementing SQL parsing/execution engine.

**Example from codebase:**

#### `mock_spark/session/sql/executor.py:59-99`
```python
def execute(self, query: str) -> IDataFrame:
    """Execute SQL query."""
    try:
        # Parse the query
        ast = self.parser.parse(query)

        # âœ‹ RAW SQL REQUIRED - We're implementing a SQL engine
        # Need to handle arbitrary user SQL
        if ast.query_type == "SELECT":
            return self._execute_select(ast)
        elif ast.query_type == "CREATE":
            return self._execute_create(ast)
        # ... more query types
```

**Why SQLModel doesn't work:**
- User provides arbitrary SQL strings
- Can't pre-parse to SQLAlchemy before execution
- Need to handle full SQL grammar

**Solutions:**

#### Option 1: Delegate to DuckDB (RECOMMENDED) âœ…
```python
from sqlalchemy import text

def execute_sql(engine, query: str):
    """Execute arbitrary SQL query."""
    with Session(engine) as session:
        # Let DuckDB handle the SQL parsing
        result = session.execute(text(query))
        return result.fetchall()
```

#### Option 2: Parse SQL and convert to SQLAlchemy âš ï¸
```python
import sqlparse
from sqlalchemy import select, insert, delete

def convert_sql_to_sqlalchemy(query: str):
    """Parse SQL and convert to SQLAlchemy."""
    parsed = sqlparse.parse(query)[0]

    # This gets complex very quickly...
    if parsed.get_type() == 'SELECT':
        # Build select() statement
        pass
    # âš ï¸ Very complex for full SQL support
```

**Recommendation:** âœ… For SQL parser implementation, use **raw SQL with `text()`**. Don't try to parse and convert.

---

### 4. âœ‹ DuckDB-Specific Features

**Issue:** DuckDB has features not in standard SQL that SQLAlchemy doesn't support.

**Examples:**

#### PRAGMA statements
```python
# âœ‹ RAW SQL REQUIRED - DuckDB-specific
connection.execute("PRAGMA memory_limit='4GB'")
connection.execute("PRAGMA threads=4")
```

#### DuckDB Extensions
```python
# âœ‹ RAW SQL REQUIRED - DuckDB-specific
connection.execute("INSTALL sqlite")
connection.execute("LOAD sqlite")
```

#### Parquet/CSV Direct Reading
```python
# âœ‹ RAW SQL REQUIRED - DuckDB-specific syntax
result = connection.execute("SELECT * FROM 'data.parquet'").fetchall()
result = connection.execute("SELECT * FROM read_csv_auto('data.csv')").fetchall()
```

**Solution:**

```python
from sqlalchemy import text

def configure_duckdb(engine):
    """Configure DuckDB-specific settings."""
    with engine.connect() as conn:
        # Raw SQL for DuckDB-specific features
        conn.execute(text("PRAGMA memory_limit='4GB'"))
        conn.execute(text("INSTALL parquet"))
        conn.execute(text("LOAD parquet"))
```

**Recommendation:** âœ… Keep raw SQL for vendor-specific features.

---

### 5. âœ‹ Performance-Critical Bulk Operations

**Issue:** Sometimes raw SQL with `executemany()` is 2-5x faster than ORM.

**Example benchmark:**

```python
import time

def benchmark_insert_methods(data: List[Dict], n=10000):
    """Compare insert performance."""

    # Method 1: SQLModel ORM (slower)
    start = time.time()
    with Session(engine) as session:
        objects = [User(**row) for row in data]
        session.add_all(objects)
        session.commit()
    orm_time = time.time() - start

    # Method 2: Raw SQL with executemany (faster)
    start = time.time()
    values = [(row['name'], row['age']) for row in data]
    connection.executemany(
        "INSERT INTO users (name, age) VALUES (?, ?)",
        values
    )
    raw_time = time.time() - start

    print(f"ORM: {orm_time:.2f}s")
    print(f"Raw SQL: {raw_time:.2f}s")
    print(f"Speedup: {orm_time/raw_time:.1f}x")

    # Result: Raw SQL often 2-5x faster for bulk inserts
```

**When to use raw SQL for performance:**

1. **Bulk inserts** (>1000 rows)
2. **Large UPDATE operations**
3. **Complex aggregations** on large datasets
4. **Data migrations**

**Solution: Hybrid approach**

```python
class DataLoader:
    """Data loader with performance optimization."""

    def insert_small_batch(self, data: List[Dict]):
        """Use SQLModel for small batches (better validation)."""
        with Session(self.engine) as session:
            objects = [User(**row) for row in data]
            session.add_all(objects)
            session.commit()

    def insert_large_batch(self, data: List[Dict]):
        """Use raw SQL for large batches (better performance)."""
        if len(data) > 1000:
            # Performance-critical: use raw SQL
            values = [(row['name'], row['age']) for row in data]
            self.connection.executemany(
                "INSERT INTO users (name, age) VALUES (?, ?)",
                values
            )
        else:
            # Small batch: use SQLModel for validation
            self.insert_small_batch(data)
```

**Recommendation:** âœ… Use raw SQL for bulk operations when performance matters.

---

### 6. âš ï¸ Complex Window Functions

**Issue:** Some advanced window functions are complex in SQLAlchemy.

**Example from codebase:**

#### `mock_spark/dataframe/sql_builder.py:149-225`
```python
def _window_function_to_sql(self, window_func: Any) -> str:
    """Convert a window function to SQL."""
    # âš ï¸ Complex but possible with SQLAlchemy

    function_name = getattr(window_func, "function_name", "window_function")
    over_clause = self._window_spec_to_sql(window_func.window_spec)

    return f"{function_name.upper()}() OVER {over_clause}"
```

**SQLAlchemy approach (more complex):**

```python
from sqlalchemy import func, over

def create_window_function(table, partition_cols, order_cols):
    """Create window function using SQLAlchemy."""
    window_spec = over(
        func.row_number(),
        partition_by=[table.c[col] for col in partition_cols],
        order_by=[table.c[col] for col in order_cols]
    )
    return window_spec
```

**When raw SQL is simpler:**

```python
# Complex window with frame specification
raw_sql = """
    SELECT *,
           SUM(salary) OVER (
               PARTITION BY dept
               ORDER BY hire_date
               ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
           ) as rolling_sum
    FROM employees
"""

# SQLAlchemy equivalent (more verbose)
from sqlalchemy import func, over

window = over(
    func.sum(table.c.salary),
    partition_by=table.c.dept,
    order_by=table.c.hire_date,
    rows=(2, 0)  # 2 PRECEDING to CURRENT ROW
)
```

**Recommendation:** âš ï¸ SQLAlchemy supports window functions but syntax is verbose. Use raw SQL for complex windows if team prefers readability.

---

### 7. âš ï¸ Dynamic Table Names

**Issue:** SQLAlchemy doesn't handle runtime table names elegantly.

**Example:**

```python
def query_dynamic_table(table_name: str):
    """Query table with name determined at runtime."""

    # âœ‹ Raw SQL (simple)
    result = connection.execute(
        f"SELECT * FROM {table_name}"  # âš ï¸ SQL injection risk!
    ).fetchall()

    # âœ… Raw SQL (safe)
    from sqlalchemy import text
    result = connection.execute(
        text(f"SELECT * FROM :table_name"),
        {"table_name": table_name}  # âš ï¸ Doesn't work! table_name can't be parameter
    ).fetchall()

    # âœ… SQLAlchemy (workaround)
    from sqlalchemy import Table, MetaData, select
    metadata = MetaData()
    table = Table(table_name, metadata, autoload_with=engine)
    stmt = select(table)
    result = session.execute(stmt).all()
```

**Recommendation:** âš ï¸ For dynamic table names, use SQLAlchemy with `autoload_with` OR sanitize table name and use raw SQL.

---

## Summary: When to Use What - **UPDATED!**

### âœ… Use SQLModel (85% of cases) ðŸ“ˆ

- **CRUD operations** on known schemas
- **Type-safe queries** with Pydantic validation
- **Joins, filters, aggregations** on predefined tables
- **Small to medium datasets**
- **Metadata operations via Inspector** â† NEW!

### âœ… Use SQLAlchemy Core (13% of cases)

- **Fully dynamic schemas** (unknown until runtime)
- **Dynamic column selection**
- **Complex query building**
- **Table reflection** with `autoload_with` â† NEW!
- **When SQLModel is too restrictive**

### âœ… Use SQLAlchemy Inspector (NEW!)

- **List tables** â†’ `inspector.get_table_names()`
- **Check table exists** â†’ `inspector.has_table(name)`
- **Get column metadata** â†’ `inspector.get_columns(name)`
- **Get schema info** â†’ `inspector.get_schema_names()`
- **Reflect existing tables** â†’ `Table(..., autoload_with=engine)`

### âœ‹ Use Raw SQL (2% of cases) ðŸ“‰

- ~~**Database metadata**~~ â† **NOW USE INSPECTOR!**
- **SQL parser implementation** (when building SQL engine)
- **Vendor-specific features** (PRAGMA, INSTALL/LOAD extensions only)
- **Performance-critical bulk operations** (>10k rows - optional)
- ~~**Dynamic table names**~~ â† **NOW USE INSPECTOR REFLECTION!**

---

## Code Examples: Hybrid Approach

### Example 1: Table Operations with Fallback

```python
class HybridTableManager:
    """Manages tables with hybrid SQL/SQLModel approach."""

    def __init__(self, engine):
        self.engine = engine
        self.metadata = MetaData()

    def create_static_table(self):
        """Use SQLModel for known schemas."""
        class User(SQLModel, table=True):
            id: Optional[int] = Field(default=None, primary_key=True)
            name: str
            age: int

        SQLModel.metadata.create_all(self.engine)
        return User

    def create_dynamic_table(self, name: str, data: List[Dict]):
        """Use SQLAlchemy Core for dynamic schemas."""
        columns = []
        for key, value in data[0].items():
            col_type = self._infer_type(value)
            columns.append(Column(key, col_type))

        table = Table(name, self.metadata, *columns)
        table.create(self.engine)
        return table

    def list_tables(self) -> List[str]:
        """Use raw SQL for metadata."""
        with self.engine.connect() as conn:
            result = conn.execute(text("SHOW TABLES"))
            return [row[0] for row in result]
```

### Example 2: Performance-Optimized Data Loader

```python
class SmartDataLoader:
    """Chooses best method based on data size."""

    BULK_THRESHOLD = 1000

    def insert(self, model: type[SQLModel], data: List[Dict]):
        """Smart insert that chooses best method."""
        if len(data) < self.BULK_THRESHOLD:
            # Small batch: use SQLModel for validation
            return self._insert_with_validation(model, data)
        else:
            # Large batch: use raw SQL for performance
            return self._insert_bulk(model.__tablename__, data)

    def _insert_with_validation(self, model, data):
        """SQLModel with Pydantic validation."""
        with Session(self.engine) as session:
            objects = [model(**row) for row in data]
            session.add_all(objects)
            session.commit()

    def _insert_bulk(self, table_name, data):
        """Raw SQL for performance."""
        if not data:
            return

        columns = list(data[0].keys())
        placeholders = ", ".join(["?" for _ in columns])
        sql = f"INSERT INTO {table_name} ({', '.join(columns)}) VALUES ({placeholders})"

        values = [tuple(row[col] for col in columns) for row in data]

        with self.engine.connect() as conn:
            conn.execute(text(sql), values)
            conn.commit()
```

---

## Recommendations for Mock Spark

### Phase 1: Replace Most SQL (Weeks 1-4)

âœ… **Use SQLModel for:**
- `storage/backends/duckdb.py` - Table creation, basic CRUD
- `dataframe/export.py` - Table creation
- Most of `sqlmodel_materializer.py`

### Phase 2: Hybrid Approach (Weeks 5-6)

âœ… **Use SQLAlchemy Core for:**
- `sql_builder.py` - Dynamic query building
- Dynamic table creation from MockSpark schemas

âœ… **Keep Raw SQL for:**
- Metadata queries (`SHOW TABLES`, `DESCRIBE`)
- SQL parser (`session/sql/executor.py`)
- DuckDB-specific features
- Bulk operations (>1000 rows)

### Guidelines

1. **Default to SQLModel** - Start with SQLModel, fall back if needed
2. **Measure performance** - If operation is slow, try raw SQL
3. **Document exceptions** - Add comment explaining why raw SQL is used
4. **Sanitize inputs** - Always use `text()` with parameters for raw SQL
5. **Test equivalence** - Verify raw SQL produces same results as ORM would

---

## Testing Raw SQL vs ORM

```python
def test_sql_equivalence():
    """Test that raw SQL and ORM produce same results."""

    # Setup
    class User(SQLModel, table=True):
        id: Optional[int] = Field(default=None, primary_key=True)
        name: str
        age: int

    engine = create_engine("duckdb:///:memory:")
    SQLModel.metadata.create_all(engine)

    # Insert test data
    with Session(engine) as session:
        session.add_all([
            User(name="Alice", age=25),
            User(name="Bob", age=30)
        ])
        session.commit()

    # Method 1: SQLModel
    with Session(engine) as session:
        orm_results = session.exec(
            select(User).where(User.age > 25)
        ).all()

    # Method 2: Raw SQL
    with engine.connect() as conn:
        raw_results = conn.execute(
            text("SELECT * FROM user WHERE age > 25")
        ).fetchall()

    # Compare
    assert len(orm_results) == len(raw_results)
    assert orm_results[0].name == raw_results[0][1]  # Compare values
```

---

## Conclusion

**SQLModel/SQLAlchemy can replace 80-95% of raw SQL** in Mock Spark, providing:
- âœ… Better type safety
- âœ… Pydantic validation
- âœ… SQL injection protection
- âœ… Database agnostic code

**But keep raw SQL for the 5-20% where it's:**
- âœ‹ **Required** (metadata, SQL parsing, vendor features)
- âš¡ **Faster** (bulk operations)
- ðŸ“– **Clearer** (complex windows, dynamic tables)

**Use a hybrid approach** - SQLModel by default, raw SQL when necessary, with clear documentation of why.

---

**Created:** October 7, 2025
**Analysis of:** Mock Spark SQL refactoring
**Version:** 1.0
