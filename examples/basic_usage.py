#!/usr/bin/env python3
"""
Basic Usage Example for Sparkless

Demonstrates core Sparkless functionality with practical examples.

Status: 515 tests passing (100%) | Production Ready | Version 2.0.0
Features: 100% Zero Raw SQL | Database Agnostic | Pure SQLAlchemy
"""

import os
import sys

# Allow running this script directly without installing the package
sys.path.append(os.path.dirname(os.path.dirname(__file__)))

from sparkless.sql import SparkSession, functions as F, Window


def main() -> None:
    """Demonstrate basic Sparkless usage."""
    # Default to fast mode unless explicitly requested to run full demo
    if os.environ.get("MOCK_SPARK_EXAMPLES_FULL") != "1":
        print("ðŸš€ Sparkless - Basic Usage Example (fast mode)")
        spark = SparkSession("BasicExampleFast")
        df = spark.createDataFrame([{"id": 1, "name": "Alice", "salary": 80000}])
        _ = df.select(
            "name", F.round(F.col("salary") / 1000, 1).alias("salary_k")
        ).collect()
        spark.stop()
        return
    print("ðŸš€ Sparkless - Basic Usage Example")
    print("=" * 60)

    # 1. Create Session
    print("\n1ï¸âƒ£  Creating Sparkless Session...")
    spark = SparkSession("BasicExample")
    print(f"   âœ“ Session created: {spark.app_name}")

    # 2. Create DataFrame
    print("\n2ï¸âƒ£  Creating DataFrame...")
    data = [
        {"id": 1, "name": "Alice", "dept": "Engineering", "salary": 80000},
        {"id": 2, "name": "Bob", "dept": "Sales", "salary": 75000},
        {"id": 3, "name": "Charlie", "dept": "Engineering", "salary": 90000},
        {"id": 4, "name": "Diana", "dept": "Marketing", "salary": 70000},
        {"id": 5, "name": "Eve", "dept": "Sales", "salary": 85000},
    ]
    df = spark.createDataFrame(data)
    print(f"   âœ“ Created DataFrame: {df.count()} rows, {len(df.columns)} columns")

    print("\n   Schema:")
    df.printSchema()

    print("\n   Data:")
    df.show()

    # 3. Filtering
    print("\n3ï¸âƒ£  Filtering...")
    high_earners = df.filter(F.col("salary") > 75000)
    print(f"   âœ“ Employees earning > $75k: {high_earners.count()}")
    high_earners.show()

    # 4. Column Operations
    print("\n4ï¸âƒ£  Column Operations...")
    result = df.select(
        "name",
        "salary",
        F.round(F.col("salary") / 1000, 1).alias("salary_k"),
        F.upper(F.col("name")).alias("upper_name"),
    )
    print("   âœ“ Applied transformations:")
    result.show()

    # 5. Aggregations
    print("\n5ï¸âƒ£  Aggregations...")
    dept_stats = (
        df.groupBy("dept")
        .agg(
            F.count("*").alias("count"),
            F.avg("salary").alias("avg_salary"),
            F.max("salary").alias("max_salary"),
        )
        .orderBy(F.desc("avg_salary"))
    )
    print("   âœ“ Department statistics:")
    dept_stats.show()

    # 6. Window Functions
    print("\n6ï¸âƒ£  Window Functions...")
    window_spec = Window.partitionBy("dept").orderBy("salary")
    ranked = df.select(
        "name", "dept", "salary", F.row_number().over(window_spec).alias("rank")
    )
    print("   âœ“ Salary rankings by department:")
    ranked.show()

    # 7. Joins
    print("\n7ï¸âƒ£  Joins...")
    dept_data = [
        {"dept": "Engineering", "location": "San Francisco"},
        {"dept": "Sales", "location": "New York"},
        {"dept": "Marketing", "location": "Boston"},
    ]
    dept_df = spark.createDataFrame(dept_data)

    joined = df.join(dept_df, "dept").select("name", "dept", "salary", "location")
    print("   âœ“ Joined with department locations:")
    joined.show()

    # 8. SQL Queries
    print("\n8ï¸âƒ£  SQL Queries...")
    df.createOrReplaceTempView("employees")
    # Simple SQL query
    sql_result = spark.sql(
        "SELECT name, dept, salary FROM employees WHERE salary > 80000"
    )
    print("   âœ“ SQL query result (salary > 80k):")
    sql_result.show()

    # 9. Lazy Evaluation Demo
    print("\n9ï¸âƒ£  Lazy Evaluation...")
    # Transformations are queued (not executed)
    lazy_result = (
        df.filter(F.col("salary") > 70000)
        .select("name", "salary")
        .orderBy(F.desc("salary"))
    )
    print("   âœ“ Transformations queued (not executed yet)")

    # Action triggers execution
    top_earners = lazy_result.collect()
    print(f"   âœ“ Action executed: {len(top_earners)} results")
    for row in top_earners:
        print(f"     - {row['name']}: ${row['salary']:,}")

    # 10. Cleanup
    print("\nðŸ”Ÿ Cleanup...")
    spark.stop()
    print("   âœ“ Session stopped")

    print("\nâœ¨ Example completed successfully!")
    print("\nðŸ’¡ Key Takeaways:")
    print("   â€¢ Sparkless provides drop-in PySpark replacement")
    print("   â€¢ No JVM required - 10x faster tests")
    print("   â€¢ Full API compatibility with lazy evaluation")
    print("   â€¢ Perfect for unit testing and CI/CD")


if __name__ == "__main__":
    main()
