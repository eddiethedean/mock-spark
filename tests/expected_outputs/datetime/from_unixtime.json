{
  "test_id": "from_unixtime",
  "pyspark_version": "3.5",
  "generated_at": "2025-10-27T14:13:25.021311",
  "input_data": [
    {
      "id": 1,
      "date": "2020-01-15",
      "timestamp": "2020-01-15 10:30:00"
    },
    {
      "id": 2,
      "date": "2019-03-10",
      "timestamp": "2019-03-10 14:20:00"
    },
    {
      "id": 3,
      "date": "2021-07-22",
      "timestamp": "2021-07-22 08:15:00"
    }
  ],
  "operation": "DataFrame operation: from_unixtime",
  "expected_output": {
    "schema": {
      "field_count": 1,
      "field_names": [
        "from_unixtime(1577836800, yyyy-MM-dd HH:mm:ss)"
      ],
      "field_types": [
        "string"
      ],
      "fields": [
        {
          "name": "from_unixtime(1577836800, yyyy-MM-dd HH:mm:ss)",
          "type": "string",
          "nullable": true
        }
      ]
    },
    "data": [
      {
        "from_unixtime(1577836800, yyyy-MM-dd HH:mm:ss)": "2019-12-31 19:00:00"
      },
      {
        "from_unixtime(1577836800, yyyy-MM-dd HH:mm:ss)": "2019-12-31 19:00:00"
      },
      {
        "from_unixtime(1577836800, yyyy-MM-dd HH:mm:ss)": "2019-12-31 19:00:00"
      }
    ],
    "row_count": 3
  }
}